{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/train.csv\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한자 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# 한자 추출 함수 정의\n",
    "def extract_hanza(text):\n",
    "    # CJK 통합 한자 범위(U+4E00 ~ U+9FFF)에 해당하는 문자 추출\n",
    "    return \"\".join(re.findall(\"[\\u4e00-\\u9fff]\", text))\n",
    "\n",
    "# 문자 빈도 축정\n",
    "def get_char_frequency(text_list):\n",
    "    # 모든 문자를 하나의 리스트로 합치기\n",
    "    chars = [char for text in text_list for char in text]\n",
    "    # Counter를 사용하여 빈도 계산\n",
    "    char_counts = Counter(chars)\n",
    "    # 빈도순으로 정렬\n",
    "    return pd.Series(char_counts).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "美    79\n",
      "北    55\n",
      "中    43\n",
      "朴    36\n",
      "靑    24\n",
      "日    21\n",
      "與    20\n",
      "文     9\n",
      "英     7\n",
      "野     6\n",
      "佛     5\n",
      "伊     5\n",
      "獨     4\n",
      "反     4\n",
      "前     3\n",
      "軍     3\n",
      "硏     3\n",
      "對     2\n",
      "外     2\n",
      "社     2\n",
      "黃     2\n",
      "亞     2\n",
      "韓     2\n",
      "株     1\n",
      "車     1\n",
      "崔     1\n",
      "院     1\n",
      "金     1\n",
      "丁     1\n",
      "小     1\n",
      "和     1\n",
      "企     1\n",
      "安     1\n",
      "展     1\n",
      "檢     1\n",
      "親     1\n",
      "銀     1\n",
      "證     1\n",
      "先     1\n",
      "父     1\n",
      "南     1\n",
      "詩     1\n",
      "家     1\n",
      "大     1\n",
      "印     1\n",
      "阿     1\n",
      "故     1\n",
      "州     1\n",
      "重     1\n",
      "dtype: int64\n",
      "49 ['美', '北', '中', '朴', '靑', '日', '與', '文', '英', '野', '佛', '伊', '獨', '反', '前', '軍', '硏', '對', '外', '社', '黃', '亞', '韓', '株', '車', '崔', '院', '金', '丁', '小', '和', '企', '安', '展', '檢', '親', '銀', '證', '先', '父', '南', '詩', '家', '大', '印', '阿', '故', '州', '重']\n"
     ]
    }
   ],
   "source": [
    "# 한자 빈도수 측정\n",
    "df[\"hanza\"] = df[\"text\"].apply(extract_hanza)\n",
    "\n",
    "char_frequency = get_char_frequency(df[\"hanza\"])\n",
    "print(char_frequency)\n",
    "hanza = list(char_frequency.keys())\n",
    "print(len(hanza), hanza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ynat-v1_train_00004</td>\n",
       "      <td>pI美대선I앞두고 R2fr단 발] $비해 감시 강화</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ynat-v1_train_00005</td>\n",
       "      <td>美성인 6명 중 1명꼴 배우자·연인 빚 떠안은 적 있다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ynat-v1_train_00008</td>\n",
       "      <td>朴대통령 얼마나 많이 놀라셨어요…경주 지진현장 방문종합</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>ynat-v1_train_00050</td>\n",
       "      <td>美MBA[여성 비율x계속 x가4주$E19a대 U입생 중Ym,%</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>ynat-v1_train_00058</td>\n",
       "      <td>한#M2 !는 유`8 치료제 오B솔 美 임{ 3a 본격화</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID                                text  target\n",
       "4   ynat-v1_train_00004        pI美대선I앞두고 R2fr단 발] $비해 감시 강화       6\n",
       "5   ynat-v1_train_00005      美성인 6명 중 1명꼴 배우자·연인 빚 떠안은 적 있다       0\n",
       "8   ynat-v1_train_00008      朴대통령 얼마나 많이 놀라셨어요…경주 지진현장 방문종합       6\n",
       "50  ynat-v1_train_00050  美MBA[여성 비율x계속 x가4주$E19a대 U입생 중Ym,%       6\n",
       "58  ynat-v1_train_00058     한#M2 !는 유`8 치료제 오B솔 美 임{ 3a 본격화       6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hanza = df[df[\"hanza\"].str.len()!=0]\n",
    "print(len(df_hanza))\n",
    "df_hanza = df_hanza[[\"ID\", \"text\", \"target\"]]\n",
    "#df_hanza.to_csv(\"hanza.csv\",index=False)\n",
    "df_hanza.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한자 전부 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_chars_from_text(df, char_list):\n",
    "    pattern = \"[\" + re.escape(\"\".join(char_list)) + \"]\"\n",
    "    df[\"filtered_text\"] = df[\"filtered_text\"].str.replace(pattern, \"\", regex=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>filtered_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ynat-v1_train_00000</td>\n",
       "      <td>정i :파1 미사z KT( 이용기간 2e 단] Q분종U2보</td>\n",
       "      <td>4</td>\n",
       "      <td>정i :파1 미사z KT( 이용기간 2e 단] Q분종U2보</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ynat-v1_train_00001</td>\n",
       "      <td>K찰.국DLwo 로L3한N% 회장 2 T0&amp;}송=</td>\n",
       "      <td>3</td>\n",
       "      <td>K찰.국DLwo 로L3한N% 회장 2 T0&amp;}송=</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ynat-v1_train_00002</td>\n",
       "      <td>m 김정) 자주통일 새,?r열1나가야1보</td>\n",
       "      <td>2</td>\n",
       "      <td>m 김정) 자주통일 새,?r열1나가야1보</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ynat-v1_train_00003</td>\n",
       "      <td>갤노트8 주말 27만대 개통…시장은 불법 보조금 얼룩</td>\n",
       "      <td>5</td>\n",
       "      <td>갤노트8 주말 27만대 개통…시장은 불법 보조금 얼룩</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ynat-v1_train_00004</td>\n",
       "      <td>pI美대선I앞두고 R2fr단 발] $비해 감시 강화</td>\n",
       "      <td>6</td>\n",
       "      <td>pI대선I앞두고 R2fr단 발] $비해 감시 강화</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID                              text  target  \\\n",
       "0  ynat-v1_train_00000  정i :파1 미사z KT( 이용기간 2e 단] Q분종U2보       4   \n",
       "1  ynat-v1_train_00001       K찰.국DLwo 로L3한N% 회장 2 T0&}송=       3   \n",
       "2  ynat-v1_train_00002            m 김정) 자주통일 새,?r열1나가야1보       2   \n",
       "3  ynat-v1_train_00003     갤노트8 주말 27만대 개통…시장은 불법 보조금 얼룩       5   \n",
       "4  ynat-v1_train_00004      pI美대선I앞두고 R2fr단 발] $비해 감시 강화       6   \n",
       "\n",
       "                      filtered_text  \n",
       "0  정i :파1 미사z KT( 이용기간 2e 단] Q분종U2보  \n",
       "1       K찰.국DLwo 로L3한N% 회장 2 T0&}송=  \n",
       "2            m 김정) 자주통일 새,?r열1나가야1보  \n",
       "3     갤노트8 주말 27만대 개통…시장은 불법 보조금 얼룩  \n",
       "4       pI대선I앞두고 R2fr단 발] $비해 감시 강화  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"filtered_text\"] = df[\"text\"].copy()\n",
    "df = remove_chars_from_text(df, hanza)\n",
    "df = df.drop(columns=[\"hanza\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자주 쓰이는 특수문자 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#영어, 숫자, 한글 제외\n",
    "def extract_special(text):\n",
    "    return \"\".join(re.findall(r\"[^a-zA-Z0-9\\sㄱ-ㅎㅏ-ㅣ가-힣]\", text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 ['…', '.', '·', '%', '\"', '-', '(', '|', '?', ',', '}', ':', '&', '_', '{', '~', '#', '\\\\', '*', ')', '$', '=', '+', '`', ';', \"'\", '!', '@', '<', '/', '>', '[', ']', '^']\n"
     ]
    }
   ],
   "source": [
    "# 특수문자 빈도수 측정\n",
    "special = df[\"filtered_text\"].apply(extract_special)\n",
    "char_frequency = get_char_frequency(special)\n",
    "filtered_chars = char_frequency[char_frequency >= 100]\n",
    "zazu_special = list(filtered_chars.keys())\n",
    "print(len(zazu_special),zazu_special)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case1. 말줄임표는 띄어쓰기로 치환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"filtered_text\"] = df[\"filtered_text\"].str.replace(\"…\", \" \")\n",
    "# # df[\"filtered_text\"] = df[\"filtered_text\"].str.replace(\n",
    "# #     \"|\".join([\"…\", \"·\", \"/\"]), \" \", regex=True\n",
    "# # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case2. 100번 이상 사용된 특수문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>filtered_text</th>\n",
       "      <th>filtered_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ynat-v1_train_00000</td>\n",
       "      <td>정i :파1 미사z KT( 이용기간 2e 단] Q분종U2보</td>\n",
       "      <td>4</td>\n",
       "      <td>정i 파1 미사z KT 이용기간 2e 단 Q분종U2보</td>\n",
       "      <td>[정i, 파1, 미사z, KT, 이용기간, 2e, 단, Q분종U2보]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ynat-v1_train_00001</td>\n",
       "      <td>K찰.국DLwo 로L3한N% 회장 2 T0&amp;}송=</td>\n",
       "      <td>3</td>\n",
       "      <td>K찰국DLwo 로L3한N 회장 2 T0송</td>\n",
       "      <td>[K찰국DLwo, 로L3한N, 회장, 2, T0송]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ynat-v1_train_00002</td>\n",
       "      <td>m 김정) 자주통일 새,?r열1나가야1보</td>\n",
       "      <td>2</td>\n",
       "      <td>m 김정 자주통일 새r열1나가야1보</td>\n",
       "      <td>[m, 김정, 자주통일, 새r열1나가야1보]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ynat-v1_train_00003</td>\n",
       "      <td>갤노트8 주말 27만대 개통…시장은 불법 보조금 얼룩</td>\n",
       "      <td>5</td>\n",
       "      <td>갤노트8 주말 27만대 개통시장은 불법 보조금 얼룩</td>\n",
       "      <td>[갤노트8, 주말, 27만대, 개통시장은, 불법, 보조금, 얼룩]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ynat-v1_train_00004</td>\n",
       "      <td>pI美대선I앞두고 R2fr단 발] $비해 감시 강화</td>\n",
       "      <td>6</td>\n",
       "      <td>pI대선I앞두고 R2fr단 발 비해 감시 강화</td>\n",
       "      <td>[pI대선I앞두고, R2fr단, 발, 비해, 감시, 강화]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID                              text  target  \\\n",
       "0  ynat-v1_train_00000  정i :파1 미사z KT( 이용기간 2e 단] Q분종U2보       4   \n",
       "1  ynat-v1_train_00001       K찰.국DLwo 로L3한N% 회장 2 T0&}송=       3   \n",
       "2  ynat-v1_train_00002            m 김정) 자주통일 새,?r열1나가야1보       2   \n",
       "3  ynat-v1_train_00003     갤노트8 주말 27만대 개통…시장은 불법 보조금 얼룩       5   \n",
       "4  ynat-v1_train_00004      pI美대선I앞두고 R2fr단 발] $비해 감시 강화       6   \n",
       "\n",
       "                   filtered_text                           filtered_list  \n",
       "0  정i 파1 미사z KT 이용기간 2e 단 Q분종U2보  [정i, 파1, 미사z, KT, 이용기간, 2e, 단, Q분종U2보]  \n",
       "1         K찰국DLwo 로L3한N 회장 2 T0송            [K찰국DLwo, 로L3한N, 회장, 2, T0송]  \n",
       "2            m 김정 자주통일 새r열1나가야1보                [m, 김정, 자주통일, 새r열1나가야1보]  \n",
       "3   갤노트8 주말 27만대 개통시장은 불법 보조금 얼룩    [갤노트8, 주말, 27만대, 개통시장은, 불법, 보조금, 얼룩]  \n",
       "4      pI대선I앞두고 R2fr단 발 비해 감시 강화        [pI대선I앞두고, R2fr단, 발, 비해, 감시, 강화]  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = remove_chars_from_text(df, zazu_special)\n",
    "df[\"filtered_list\"] = df[\"filtered_text\"].apply(str.split)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태소 분석기를 통한 한글+영문+숫자+특문 혼합 단어 탐지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install KoNLPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분석기 임포트\n",
    "from konlpy.tag import *\n",
    "\n",
    "# 각 분석기 객체 생성\n",
    "hannanum = Hannanum()\n",
    "kkma = Kkma()\n",
    "komoran = Komoran()\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hannanum\"] = df[\"filtered_list\"].apply(lambda li: [hannanum.pos(e) for e in li])\n",
    "df[\"kkma\"] = df[\"filtered_list\"].apply(lambda li: [kkma.pos(e) for e in li])\n",
    "df[\"komoran\"] = df[\"filtered_list\"].apply(lambda li: [komoran.pos(e) for e in li])\n",
    "df[\"okt\"] = df[\"filtered_list\"].apply(lambda li: [okt.pos(e) for e in li])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 문장에서 조건 탐지\n",
    "def check_tags_count_V1(morpheme_lists):\n",
    "    # 단어에서 조건 탐지\n",
    "    def check_tags_count_word(sublist):\n",
    "        tag_counts = Counter(tag for _, tag in sublist)\n",
    "\n",
    "        alpha = tag_counts[\"Alpha\"]\n",
    "        punctuation = tag_counts[\"Punctuation\"]\n",
    "        number = tag_counts[\"Number\"]\n",
    "        rest = sum(tag_counts.values()) - (alpha + punctuation + number)\n",
    "\n",
    "        # 조건 리스트\n",
    "        conditions = [\n",
    "            alpha >= 1 and punctuation >= 1 and number >= 1,\n",
    "            alpha >= 2,\n",
    "            punctuation >= 2,\n",
    "            # number >= 2,\n",
    "        ]\n",
    "\n",
    "        return any(conditions)\n",
    "\n",
    "    # 하나의 단어라도 조건을 만족하면 True\n",
    "    return any(check_tags_count_word(sublist) for sublist in morpheme_lists)\n",
    "\n",
    "\n",
    "# 데이터프레임에 적용\n",
    "df[\"check\"] = df[\"okt\"].apply(check_tags_count_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ynat-v1_train_00000</td>\n",
       "      <td>4</td>\n",
       "      <td>정i :파1 미사z KT( 이용기간 2e 단] Q분종U2보</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ynat-v1_train_00001</td>\n",
       "      <td>3</td>\n",
       "      <td>K찰.국DLwo 로L3한N% 회장 2 T0&amp;}송=</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ynat-v1_train_00004</td>\n",
       "      <td>6</td>\n",
       "      <td>pI美대선I앞두고 R2fr단 발] $비해 감시 강화</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ynat-v1_train_00010</td>\n",
       "      <td>5</td>\n",
       "      <td>oi 매력 R모h츠a열#w3약 &gt;l·주가 고Q/진</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ynat-v1_train_00018</td>\n",
       "      <td>0</td>\n",
       "      <td>개R전 연w정연H 작가</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID  target                              text\n",
       "0   ynat-v1_train_00000       4  정i :파1 미사z KT( 이용기간 2e 단] Q분종U2보\n",
       "1   ynat-v1_train_00001       3       K찰.국DLwo 로L3한N% 회장 2 T0&}송=\n",
       "4   ynat-v1_train_00004       6      pI美대선I앞두고 R2fr단 발] $비해 감시 강화\n",
       "10  ynat-v1_train_00010       5       oi 매력 R모h츠a열#w3약 >l·주가 고Q/진\n",
       "18  ynat-v1_train_00018       0                      개R전 연w정연H 작가"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_morph_condition_V1 = df[df[\"check\"]==True]\n",
    "df_morph_condition_V1 = df_morph_condition_V1[[\"ID\", \"target\", \"text\"]]\n",
    "df_morph_condition_V1.to_csv(\"df_morph_condition_V1.csv\",index=False)\n",
    "print(len(df_morph_condition_V1))\n",
    "df_morph_condition_V1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### morph 패턴 V1 기반 1118개 실제 노이즈 탐지, 2개 오탐지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1680"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[\"check\"] == False]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 통째로 탐색\n",
    "def check_tags_count_V2(morpheme_lists):\n",
    "    # 태그 카운트\n",
    "    all_tags = [tag for sublist in morpheme_lists for _, tag in sublist]\n",
    "    tag_counts = Counter(all_tags)\n",
    "\n",
    "    alpha = tag_counts[\"Alpha\"]\n",
    "    punctuation = tag_counts[\"Punctuation\"]\n",
    "    number = tag_counts[\"Number\"]\n",
    "    rest = sum(tag_counts.values()) - (alpha + punctuation + number)\n",
    "\n",
    "    # 조건 리스트\n",
    "    conditions = []\n",
    "    # conditions.append(punctuation >= 1)\n",
    "    conditions.append(alpha + punctuation + number >= 6)\n",
    "    return all(conditions)\n",
    "\n",
    "# 데이터프레임에 적용\n",
    "df[\"check\"] = df[\"okt\"].apply(check_tags_count_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ynat-v1_train_00013</td>\n",
       "      <td>4</td>\n",
       "      <td>아이`XSI수리0* b대`…맥3 디dF레&lt; 41/'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ynat-v1_train_00014</td>\n",
       "      <td>2</td>\n",
       "      <td>문/인 당2 4nS 민관2동7사위 /\"X보 철거tt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>ynat-v1_train_00054</td>\n",
       "      <td>4</td>\n",
       "      <td>찍W ,fK는 즐거m T졌다…5개 카메: LG V40 z큐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>ynat-v1_train_00058</td>\n",
       "      <td>6</td>\n",
       "      <td>한#M2 !는 유`8 치료제 오B솔 美 임{ 3a 본격화</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>ynat-v1_train_00094</td>\n",
       "      <td>1</td>\n",
       "      <td>멀티골 j순형 u로축구 pB그1 1\"\\운드FXVP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID  target                              text\n",
       "13  ynat-v1_train_00013       4      아이`XSI수리0* b대`…맥3 디dF레< 41/'\n",
       "14  ynat-v1_train_00014       2      문/인 당2 4nS 민관2동7사위 /\"X보 철거tt\n",
       "54  ynat-v1_train_00054       4  찍W ,fK는 즐거m T졌다…5개 카메: LG V40 z큐\n",
       "58  ynat-v1_train_00058       6   한#M2 !는 유`8 치료제 오B솔 美 임{ 3a 본격화\n",
       "94  ynat-v1_train_00094       1       멀티골 j순형 u로축구 pB그1 1\"\\운드FXVP"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_morph_condition_V2 = df[df[\"check\"] == True]\n",
    "df_morph_condition_V2 = df_morph_condition_V2[[\"ID\", \"target\", \"text\"]]\n",
    "df_morph_condition_V2.to_csv(\"df_morph_condition_V2.csv\", index=False)\n",
    "print(len(df_morph_condition_V2))\n",
    "df_morph_condition_V2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### morph 패턴 V2 기반 96개 실제 노이즈 탐지, 5개 오탐지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1579"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[\"check\"] == False]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 샌드위치 패턴\n",
    "한글 사이에 낀 문자 중 (한글, 영어 대문자, 숫자, 관계 표현 특수문자)를 제외한 문자들 탐지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sandwich_pattern(text_list):\n",
    "    return any(\n",
    "        bool(re.search(r\"[가-힣]+[^A-Z가-힣0-9ㆍ><∼→←↑↓↔]+[가-힣]+\", str(item)))\n",
    "        for item in text_list\n",
    "    )\n",
    "\n",
    "df[\"sandwich\"] = df[\"filtered_list\"].apply(detect_sandwich_pattern)\n",
    "sandwich_df = df[df[\"sandwich\"]==True]\n",
    "not_sandwich_df = df[df[\"sandwich\"] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121 1458\n"
     ]
    }
   ],
   "source": [
    "print(len(sandwich_df),len(not_sandwich_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandwich_df = sandwich_df[[\"ID\", \"target\", \"text\"]]\n",
    "sandwich_df.to_csv(\"sandwich_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 샌드위치 패턴 기반 121개 실제 노이즈 탐지. 0개 오탐지."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1458"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = not_sandwich_df\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 성재형 방법 (st_pages/noise_viz.py 참고) 문자 비율 기반 노이즈 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 한글.숫자,영어 대문자 기반의 노이즈 탐색\n",
    "def calculate_noise_ratio(text):\n",
    "    if len(text)==0:\n",
    "        return 0\n",
    "    return round((len(re.findall(r\"[^A-Z0-9가-힣\\s]\", text)) / len(text)), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, \"noise_ratio\"] = df[\"filtered_text\"].apply(calculate_noise_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ynat-v1_train_00006</td>\n",
       "      <td>1</td>\n",
       "      <td>프로야구~롯TKIAs광주 경기 y천취소</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ynat-v1_train_00030</td>\n",
       "      <td>4</td>\n",
       "      <td>해외로밍 m금폭탄 n동차단 더 빨$진다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>ynat-v1_train_00053</td>\n",
       "      <td>3</td>\n",
       "      <td>코로나 r대^등교)모습</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>ynat-v1_train_00101</td>\n",
       "      <td>1</td>\n",
       "      <td>~학농구리- 8일 고려대중앙대 경pt 개막</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>ynat-v1_train_00136</td>\n",
       "      <td>2</td>\n",
       "      <td>xW리 a)엔 예비후0V사전여론조사 결과 유출 논c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ID  target                          text\n",
       "6    ynat-v1_train_00006       1         프로야구~롯TKIAs광주 경기 y천취소\n",
       "30   ynat-v1_train_00030       4         해외로밍 m금폭탄 n동차단 더 빨$진다\n",
       "53   ynat-v1_train_00053       3                  코로나 r대^등교)모습\n",
       "101  ynat-v1_train_00101       1       ~학농구리- 8일 고려대중앙대 경pt 개막\n",
       "136  ynat-v1_train_00136       2  xW리 a)엔 예비후0V사전여론조사 결과 유출 논c"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_special_character = df[df[\"noise_ratio\"] >= 0.09]\n",
    "print(len(high_special_character))\n",
    "high_special_character = high_special_character.sort_values(\n",
    "    by=[\"noise_ratio\"], axis=0, ascending=True\n",
    ")\n",
    "\n",
    "high_special_character = high_special_character[[\"ID\", \"target\", \"text\"]]\n",
    "high_special_character = high_special_character.sort_values(by=\"ID\", ascending=True)\n",
    "high_special_character.to_csv(\"high_special_character.csv\", index=False)\n",
    "high_special_character.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1365\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>ynat-v1_train_01201</td>\n",
       "      <td>2</td>\n",
       "      <td>여야 ~종인 개헌특l 제안에 ~갈v -응&amp;험로 예'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>ynat-v1_train_00615</td>\n",
       "      <td>4</td>\n",
       "      <td>티맥, 인d·AI a업지능으로  즈니(·생활 혁신</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2604</th>\n",
       "      <td>ynat-v1_train_02604</td>\n",
       "      <td>6</td>\n",
       "      <td>브_질A언론 {세프 V통. 탄핵rM능성 더 커j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>ynat-v1_train_02592</td>\n",
       "      <td>2</td>\n",
       "      <td>[ '대통령재벌 총수 u공&amp; 면담 경위 수사종b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>ynat-v1_train_02473</td>\n",
       "      <td>1</td>\n",
       "      <td>v준우 \\인 성적 필요 *어…5위 F[만 g각한다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ID  target                          text\n",
       "1201  ynat-v1_train_01201       2  여야 ~종인 개헌특l 제안에 ~갈v -응&험로 예'\n",
       "615   ynat-v1_train_00615       4   티맥, 인d·AI a업지능으로  즈니(·생활 혁신\n",
       "2604  ynat-v1_train_02604       6    브_질A언론 {세프 V통. 탄핵rM능성 더 커j\n",
       "2592  ynat-v1_train_02592       2    [ '대통령재벌 총수 u공& 면담 경위 수사종b\n",
       "2473  ynat-v1_train_02473       1   v준우 \\인 성적 필요 *어…5위 F[만 g각한다"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_special_character = df[df[\"noise_ratio\"]<0.09]\n",
    "print(len(less_special_character))\n",
    "less_special_character = less_special_character.sort_values(\n",
    "    by=[\"noise_ratio\"], axis=0, ascending=False\n",
    ")\n",
    "less_special_character = less_special_character[[\"ID\", \"target\", \"text\"]]\n",
    "less_special_character.to_csv(\"rest.csv\", index=False)\n",
    "less_special_character.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문자 비율 기반 93개 실제 노이즈 탐지. 4개 오탐지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1365"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[\"noise_ratio\"] < 0.09]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>filtered_text</th>\n",
       "      <th>filtered_list</th>\n",
       "      <th>hannanum</th>\n",
       "      <th>kkma</th>\n",
       "      <th>komoran</th>\n",
       "      <th>okt</th>\n",
       "      <th>check</th>\n",
       "      <th>sandwich</th>\n",
       "      <th>noise_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ynat-v1_train_00003</td>\n",
       "      <td>갤노트8 주말 27만대 개통…시장은 불법 보조금 얼룩</td>\n",
       "      <td>5</td>\n",
       "      <td>갤노트8 주말 27만대 개통시장은 불법 보조금 얼룩</td>\n",
       "      <td>[갤노트8, 주말, 27만대, 개통시장은, 불법, 보조금, 얼룩]</td>\n",
       "      <td>[[(갤노트8, N)], [(주말, N)], [(27만대, N)], [(개통시장, ...</td>\n",
       "      <td>[[(개, VV), (ㄹ, ETD), (노트, NNG), (8, NR)], [(주말...</td>\n",
       "      <td>[[(개, VV), (ㄹ, ETM), (노트, NNP), (8, SN)], [(주말...</td>\n",
       "      <td>[[(갤, Verb), (노트, Noun), (8, Number)], [(주말, N...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ynat-v1_train_00005</td>\n",
       "      <td>美성인 6명 중 1명꼴 배우자·연인 빚 떠안은 적 있다</td>\n",
       "      <td>0</td>\n",
       "      <td>성인 6명 중 1명꼴 배우자연인 빚 떠안은 적 있다</td>\n",
       "      <td>[성인, 6명, 중, 1명꼴, 배우자연인, 빚, 떠안은, 적, 있다]</td>\n",
       "      <td>[[(성인, N)], [(6명, N)], [(중, N)], [(1명, N), (꼴,...</td>\n",
       "      <td>[[(성인, NNG)], [(6, NR), (명, NNM)], [(중, NNG)],...</td>\n",
       "      <td>[[(성인, NNG)], [(6, SN), (명, NNB)], [(중, NNB)],...</td>\n",
       "      <td>[[(성인, Noun)], [(6, Number), (명, Noun)], [(중, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ynat-v1_train_00007</td>\n",
       "      <td>아가메즈 33득점 우리카드 KB손해보험 완파…3위 굳...</td>\n",
       "      <td>4</td>\n",
       "      <td>아가메즈 33득점 우리카드 KB손해보험 완파3위 굳</td>\n",
       "      <td>[아가메즈, 33득점, 우리카드, KB손해보험, 완파3위, 굳]</td>\n",
       "      <td>[[(아가메즈, N)], [(33득점, N)], [(우리카드, N)], [(KB, ...</td>\n",
       "      <td>[[(아가, NNG), (메, NNG), (즈, UN)], [(33, NR), (득...</td>\n",
       "      <td>[[(아가메즈, NA)], [(33, SN), (득점, NNP)], [(우리카드, ...</td>\n",
       "      <td>[[(아가, Noun), (메, Noun), (즈, Modifier)], [(33,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ynat-v1_train_00008</td>\n",
       "      <td>朴대통령 얼마나 많이 놀라셨어요…경주 지진현장 방문종합</td>\n",
       "      <td>6</td>\n",
       "      <td>대통령 얼마나 많이 놀라셨어요경주 지진현장 방문종합</td>\n",
       "      <td>[대통령, 얼마나, 많이, 놀라셨어요경주, 지진현장, 방문종합]</td>\n",
       "      <td>[[(대통령, N)], [(얼마나, M)], [(많, P), (이, X)], [(놀...</td>\n",
       "      <td>[[(대통령, NNG)], [(얼마나, MAG)], [(많이, MAG)], [(놀라...</td>\n",
       "      <td>[[(대통령, NNG)], [(얼마나, MAG)], [(많이, MAG)], [(놀라...</td>\n",
       "      <td>[[(대통령, Noun)], [(얼마나, Noun)], [(많이, Adverb)],...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ynat-v1_train_00009</td>\n",
       "      <td>듀얼심 아이폰 하반기 출시설 솔솔…알뜰폰 기대감</td>\n",
       "      <td>4</td>\n",
       "      <td>듀얼심 아이폰 하반기 출시설 솔솔알뜰폰 기대감</td>\n",
       "      <td>[듀얼심, 아이폰, 하반기, 출시설, 솔솔알뜰폰, 기대감]</td>\n",
       "      <td>[[(듀얼심, N)], [(아이폰, N)], [(하반기, N)], [(출시설, N)...</td>\n",
       "      <td>[[(듀얼, NNG), (심, NNG)], [(아이, NNG), (폰, NNG)],...</td>\n",
       "      <td>[[(듀얼심, NA)], [(아이폰, NNP)], [(하반기, NNG)], [(출시...</td>\n",
       "      <td>[[(듀얼, Noun), (심, Noun)], [(아이폰, Noun)], [(하반기...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID                              text  target  \\\n",
       "3  ynat-v1_train_00003     갤노트8 주말 27만대 개통…시장은 불법 보조금 얼룩       5   \n",
       "5  ynat-v1_train_00005    美성인 6명 중 1명꼴 배우자·연인 빚 떠안은 적 있다       0   \n",
       "7  ynat-v1_train_00007  아가메즈 33득점 우리카드 KB손해보험 완파…3위 굳...       4   \n",
       "8  ynat-v1_train_00008    朴대통령 얼마나 많이 놀라셨어요…경주 지진현장 방문종합       6   \n",
       "9  ynat-v1_train_00009        듀얼심 아이폰 하반기 출시설 솔솔…알뜰폰 기대감       4   \n",
       "\n",
       "                  filtered_text                           filtered_list  \\\n",
       "3  갤노트8 주말 27만대 개통시장은 불법 보조금 얼룩    [갤노트8, 주말, 27만대, 개통시장은, 불법, 보조금, 얼룩]   \n",
       "5  성인 6명 중 1명꼴 배우자연인 빚 떠안은 적 있다  [성인, 6명, 중, 1명꼴, 배우자연인, 빚, 떠안은, 적, 있다]   \n",
       "7  아가메즈 33득점 우리카드 KB손해보험 완파3위 굳     [아가메즈, 33득점, 우리카드, KB손해보험, 완파3위, 굳]   \n",
       "8  대통령 얼마나 많이 놀라셨어요경주 지진현장 방문종합     [대통령, 얼마나, 많이, 놀라셨어요경주, 지진현장, 방문종합]   \n",
       "9     듀얼심 아이폰 하반기 출시설 솔솔알뜰폰 기대감        [듀얼심, 아이폰, 하반기, 출시설, 솔솔알뜰폰, 기대감]   \n",
       "\n",
       "                                            hannanum  \\\n",
       "3  [[(갤노트8, N)], [(주말, N)], [(27만대, N)], [(개통시장, ...   \n",
       "5  [[(성인, N)], [(6명, N)], [(중, N)], [(1명, N), (꼴,...   \n",
       "7  [[(아가메즈, N)], [(33득점, N)], [(우리카드, N)], [(KB, ...   \n",
       "8  [[(대통령, N)], [(얼마나, M)], [(많, P), (이, X)], [(놀...   \n",
       "9  [[(듀얼심, N)], [(아이폰, N)], [(하반기, N)], [(출시설, N)...   \n",
       "\n",
       "                                                kkma  \\\n",
       "3  [[(개, VV), (ㄹ, ETD), (노트, NNG), (8, NR)], [(주말...   \n",
       "5  [[(성인, NNG)], [(6, NR), (명, NNM)], [(중, NNG)],...   \n",
       "7  [[(아가, NNG), (메, NNG), (즈, UN)], [(33, NR), (득...   \n",
       "8  [[(대통령, NNG)], [(얼마나, MAG)], [(많이, MAG)], [(놀라...   \n",
       "9  [[(듀얼, NNG), (심, NNG)], [(아이, NNG), (폰, NNG)],...   \n",
       "\n",
       "                                             komoran  \\\n",
       "3  [[(개, VV), (ㄹ, ETM), (노트, NNP), (8, SN)], [(주말...   \n",
       "5  [[(성인, NNG)], [(6, SN), (명, NNB)], [(중, NNB)],...   \n",
       "7  [[(아가메즈, NA)], [(33, SN), (득점, NNP)], [(우리카드, ...   \n",
       "8  [[(대통령, NNG)], [(얼마나, MAG)], [(많이, MAG)], [(놀라...   \n",
       "9  [[(듀얼심, NA)], [(아이폰, NNP)], [(하반기, NNG)], [(출시...   \n",
       "\n",
       "                                                 okt  check  sandwich  \\\n",
       "3  [[(갤, Verb), (노트, Noun), (8, Number)], [(주말, N...  False     False   \n",
       "5  [[(성인, Noun)], [(6, Number), (명, Noun)], [(중, ...  False     False   \n",
       "7  [[(아가, Noun), (메, Noun), (즈, Modifier)], [(33,...  False     False   \n",
       "8  [[(대통령, Noun)], [(얼마나, Noun)], [(많이, Adverb)],...  False     False   \n",
       "9  [[(듀얼, Noun), (심, Noun)], [(아이폰, Noun)], [(하반기...  False     False   \n",
       "\n",
       "   noise_ratio  \n",
       "3          0.0  \n",
       "5          0.0  \n",
       "7          0.0  \n",
       "8          0.0  \n",
       "9          0.0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연속 특수문자 패턴으로 탐지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_469324/349284051.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, \"special\"] = df[\"filtered_text\"].apply(find_back_to_back_special)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 특수문자 연속으로 나오는 패턴 중 실제로 사용되는 의미있는 패턴\n",
    "df.loc[:, \"filtered_text\"] = df[\"text\"].copy()\n",
    "df.loc[:, \"filtered_text\"] = df[\"filtered_text\"].str.replace(\"…\", \" \")\n",
    "df.loc[:, \"filtered_text\"] = df[\"filtered_text\"].str.replace(\"...\", \"\")\n",
    "df.loc[:, \"filtered_text\"] = df[\"filtered_text\"].str.replace(\"..\", \"\")\n",
    "df.loc[:, \"filtered_text\"] = df[\"filtered_text\"].str.replace(\"%↑\", \"\")\n",
    "df.loc[:, \"filtered_text\"] = df[\"filtered_text\"].str.replace(\"%↓\", \"\")\n",
    "df.loc[:, \"filtered_text\"] = df[\"filtered_text\"].str.replace(\"%→\", \"\")\n",
    "df.loc[:, \"filtered_text\"] = df[\"filtered_text\"].str.replace(\"%←\", \"\")\n",
    "\n",
    "# 띄어쓰기를 제외한 특수문자가 2회 이상 연속이면 True\n",
    "def find_back_to_back_special(text):\n",
    "    return bool(re.search(r\"[^A-Za-z0-9가-힣\\s\\u4e00-\\u9fff]{2,}\", text))\n",
    "\n",
    "\n",
    "df.loc[:, \"special\"] = df[\"filtered_text\"].apply(find_back_to_back_special)\n",
    "back_to_back_special = df[df[\"special\"] == True]\n",
    "print(len(back_to_back_special))\n",
    "back_to_back_special = back_to_back_special[[\"ID\", \"target\", \"text\"]]\n",
    "back_to_back_special.to_csv(\"back_to_back_special.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1326\n"
     ]
    }
   ],
   "source": [
    "df = df[df[\"special\"] == False]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연속 특수문자 기반 39개 실제 노이즈 탐지. 0개 오탐지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수진 누나 방법 - noise_eda_special_char.ipynb 참고하여 탐지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def specical_char_ratio(text):\n",
    "    noise_count = len(re.findall(r\"[^a-zA-Z0-9\\sㄱ-ㅎㅏ-ㅣ가-힣一-龥]\", text))\n",
    "    return noise_count / len(text) if len(text) > 0 else 0\n",
    "\n",
    "\n",
    "# 소문자 알파벳 비율\n",
    "def alphabet_ratio(text):\n",
    "    alphabet_count = len(re.findall(r\"[a-z]\", text))\n",
    "    return alphabet_count / len(text) if len(text) > 0 else 0\n",
    "\n",
    "\n",
    "# 대소문자 알파벳 비율\n",
    "def all_alphabet_ratio(text):\n",
    "    alphabet_count = len(re.findall(r\"[A-Za-z]\", text))\n",
    "    return alphabet_count / len(text) if len(text) > 0 else 0\n",
    "\n",
    "\n",
    "# 특수문자 제거\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub(r\"[!#$\\'\\\"\\(\\)*+\\-/:;<=>?@\\[\\\\\\]^_`{|}&]+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def save_dataframe_to_csv(dataframe, output_path, index=False):\n",
    "    dataframe.to_csv(output_path, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "1203\n"
     ]
    }
   ],
   "source": [
    "## 특수문자가 하나도 제거되지 않은 경우 노이즈가 추가되지 않았을 확률이 높은 데이터라고 가정한다.\n",
    "\n",
    "# 특수문자 제거\n",
    "df.loc[:, \"remove_special_char\"] = df[\"text\"].apply(remove_special_characters)\n",
    "df.head(5)\n",
    "\n",
    "# text와 특수문자제거 text의 길이 차이를 확인\n",
    "df.loc[:, \"text_length\"] = df[\"text\"].apply(len)\n",
    "df.loc[:, \"remove_special_char_length\"] = df[\"remove_special_char\"].apply(len)\n",
    "df.loc[:, \"length_diff\"] = df[\"text_length\"] - df[\"remove_special_char_length\"]\n",
    "\n",
    "# 특수문자가 제거되지 않은 경우\n",
    "length_diff_0 = df[df[\"length_diff\"] == 0].copy()\n",
    "\n",
    "# 특수문자가 제거되지 않은 경우 소문자의 비율을 확인한다.\n",
    "length_diff_0.loc[:, \"alphabet_ratio\"] = length_diff_0[\"remove_special_char\"].apply(alphabet_ratio)\n",
    "\n",
    "sorted_length_diff_0 = length_diff_0.sort_values(by=\"alphabet_ratio\", ascending=False)\n",
    "no_noise_df = sorted_length_diff_0[sorted_length_diff_0[\"alphabet_ratio\"] < 0.1]\n",
    "\n",
    "noise_data1 = df[df[\"length_diff\"] != 0]\n",
    "noise_data1 = noise_data1[[\"ID\", \"text\", \"target\"]]\n",
    "\n",
    "noise_data2 = sorted_length_diff_0[sorted_length_diff_0[\"alphabet_ratio\"] >= 0.1]\n",
    "noise_data2 = noise_data2[[\"ID\", \"text\", \"target\"]]\n",
    "\n",
    "noise_df = pd.concat([noise_data1, noise_data2])\n",
    "print(len(noise_df))\n",
    "noise_df.to_csv(\"special_condition_based_noise.csv\",index=False)\n",
    "\n",
    "no_noise_df = no_noise_df[[\"ID\", \"text\", \"target\"]]\n",
    "print(len(no_noise_df))\n",
    "no_noise_df.to_csv(\"special_condition_based_not_noise.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특수문자 및 소문자 비율 기반 123개 실제 노이즈 탐지. 0개 오탐지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1597\n",
      "1203\n"
     ]
    }
   ],
   "source": [
    "dfs = [\n",
    "    pd.read_csv(\"df_morph_condition_V1.csv\"),\n",
    "    pd.read_csv(\"df_morph_condition_V2.csv\"),\n",
    "    pd.read_csv(\"sandwich_df.csv\"),\n",
    "    pd.read_csv(\"high_special_character.csv\"),\n",
    "    pd.read_csv(\"back_to_back_special.csv\"),\n",
    "    pd.read_csv(\"special_condition_based_noise.csv\"),\n",
    "]\n",
    "\n",
    "rule_based_noise = pd.concat(dfs)\n",
    "print(len(rule_based_noise))\n",
    "rule_based_noise = rule_based_noise[[\"ID\", \"target\", \"text\"]]\n",
    "rule_based_noise = rule_based_noise.sort_values(by=\"ID\", ascending=True)\n",
    "rule_based_noise.to_csv(\"rule_based_noise.csv\", index=False)\n",
    "\n",
    "rule_based_not_noise = no_noise_df.copy()\n",
    "print(len(rule_based_not_noise))\n",
    "rule_based_not_noise.to_csv(\"rule_based_not_noise.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 기반 노이즈 탐지\n",
    "1. 현재 노이즈 탐지한 데이터 1597개, 남은 데이터 1203개 존재한다.\n",
    "2. 1597개 라벨 1(노이즈)을 부여하고, 남은 데이터 1203개 라벨 0(비 노이즈)를 부여하고 두개의 셋으로 쪼갠다.\n",
    "3. 라벨 1 + 라벨 0 절반을 학습 셋, 나머지 라벨 0 절반을 테스트 셋으로 만든다. (라벨0 + 라벨1-V1)+(라벨1-V2)와 (라벨0 + 라벨1-V2)+(라벨1-V1) 이렇게 두 쌍을 만들 수 있다. \n",
    "4. 두번의 학습&추론 과정으로 남은 데이터셋에 대한 노이즈 탐색을 수행할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 전처리: 한자 제거 및 자주 쓰이는(통계 기반) 특수문자 제거\n",
    "# 그리고 뉴스 기사에서 자주 쓰이는(경험 기반) 의미있는 특수문자 제거\n",
    "rule_based_not_noise[\"filtered_text\"] = rule_based_not_noise[\"text\"].copy()\n",
    "rule_based_not_noise = remove_chars_from_text(rule_based_not_noise, hanza)\n",
    "rule_based_not_noise = remove_chars_from_text(\n",
    "    rule_based_not_noise, zazu_special + [\"％\", \"↑\", \"↓\", \"→\", \"←\"]\n",
    ")\n",
    "\n",
    "rule_based_noise[\"filtered_text\"] = rule_based_noise[\"text\"].copy()\n",
    "rule_based_noise = remove_chars_from_text(rule_based_noise, hanza)\n",
    "rule_based_not_noise = remove_chars_from_text(rule_based_not_noise, zazu_special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_based_not_noise.loc[:, \"label\"] = 0\n",
    "rule_based_noise.loc[:, \"label\"] = 1\n",
    "\n",
    "# 추론할 데이터\n",
    "rule_based_not_noise_V1 = rule_based_not_noise[: len(rule_based_not_noise)//2].copy()\n",
    "rule_based_not_noise_V2 = rule_based_not_noise[len(rule_based_not_noise)//2 :].copy()\n",
    "\n",
    "# 훈련에 사용할 데이터\n",
    "nd_train_V1 = pd.concat([rule_based_noise, rule_based_not_noise_V1])\n",
    "nd_train_V2 = pd.concat([rule_based_noise, rule_based_not_noise_V2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "# Set seed before any other operations\n",
    "set_seed(42)  # You can change this seed value as needed\n",
    "\n",
    "\n",
    "class NoiseTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=64):\n",
    "        self.texts = df[\"filtered_text\"].values\n",
    "        self.labels = df[\"label\"].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def main(model_name, df_train, df_test, output_path):\n",
    "    # Set seed at the start of main function as well\n",
    "    set_seed(42)\n",
    "\n",
    "    # 모델과 토크나이저 초기화\n",
    "    # model_name = \"klue/roberta-large\" \"klue/bert-base\" \"FacebookAI/xlm-roberta-large\" \"FacebookAI/xlm-roberta-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "    # 데이터셋과 데이터로더 설정\n",
    "    dataset = NoiseTextDataset(df_train, tokenizer)\n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=True,\n",
    "        worker_init_fn=lambda worker_id: np.random.seed(42),\n",
    "    )\n",
    "\n",
    "    # 학습 설정\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    # 학습 실행\n",
    "    num_epochs = 3\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # 예측 함수\n",
    "    def predict_text(text):\n",
    "        model.eval()\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=64,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = encoding[\"input_ids\"].to(device)\n",
    "        attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            _, predicted = torch.max(predictions, dim=1)\n",
    "\n",
    "        return 1 if predicted.item() == 1 else 0\n",
    "\n",
    "    # 테스트 데이터에 대한 예측 결과 저장\n",
    "    predictions = []\n",
    "    for test_text in df_test[\"filtered_text\"]:\n",
    "        pred = predict_text(test_text)\n",
    "        predictions.append(pred)\n",
    "\n",
    "    # 예측 결과를 데이터프레임에 추가\n",
    "    df_test[\"predicted\"] = predictions\n",
    "\n",
    "    # 결과 저장\n",
    "    df_test.to_csv(output_path, index=False)\n",
    "    print(f\"Results saved to {output_path}\")\n",
    "\n",
    "    # 예측 결과 요약 출력\n",
    "    print(\"\\nPrediction Summary:\")\n",
    "    print(f\"Total samples: {len(df_test)}\")\n",
    "    print(f\"Predicted noise texts: {sum(predictions)}\")\n",
    "    print(f\"Predicted normal texts: {len(predictions) - sum(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.0819\n",
      "Epoch 2/3, Loss: 0.0386\n",
      "Epoch 3/3, Loss: 0.0408\n",
      "Results saved to noise_detect_result_1.csv\n",
      "\n",
      "Prediction Summary:\n",
      "Total samples: 602\n",
      "Predicted noise texts: 0\n",
      "Predicted normal texts: 602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.0589\n",
      "Epoch 2/3, Loss: 0.0319\n",
      "Epoch 3/3, Loss: 0.0113\n",
      "Results saved to noise_detect_result_2.csv\n",
      "\n",
      "Prediction Summary:\n",
      "Total samples: 601\n",
      "Predicted noise texts: 9\n",
      "Predicted normal texts: 592\n"
     ]
    }
   ],
   "source": [
    "main(\"klue/roberta-large\", nd_train_V1, rule_based_not_noise_V2, \"noise_detect_result_1.csv\")\n",
    "main(\"klue/roberta-large\", nd_train_V2, rule_based_not_noise_V1, \"noise_detect_result_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_predict_1 = pd.read_csv(\"./noise_detect_result_1.csv\")\n",
    "df_model_predict_2 = pd.read_csv(\"./noise_detect_result_2.csv\")\n",
    "df_model_predict = pd.concat([df_model_predict_1, df_model_predict_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "df_model_based_noise = df_model_predict[df_model_predict[\"predicted\"] == 1]\n",
    "print(len(df_model_based_noise))\n",
    "df_model_based_noise = df_model_based_noise.sort_values(by=\"ID\", ascending=True)\n",
    "df_model_based_noise.to_csv(\"df_model_based_noise.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1194\n"
     ]
    }
   ],
   "source": [
    "df_model_based_not_noise = df_model_predict[df_model_predict[\"predicted\"] == 0]\n",
    "df_model_based_not_noise = df_model_based_not_noise[[\"ID\", \"target\", \"text\"]]\n",
    "print(len(df_model_based_not_noise))\n",
    "df_model_based_not_noise = df_model_based_not_noise.sort_values(by=\"ID\", ascending=True)\n",
    "df_model_based_not_noise.to_csv(\"df_model_based_not_noise.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 기반 9개 실제 노이즈 탐지. 0개 오탐지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 한번 더 사용하여 탐지하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 최종 결과: 오탐지 데이터 (14개)\n",
    "- 노이즈 데이터 1606개 (실제 노이즈 1592개 + 비노이즈 데이터 14개)  \n",
    "- 나머지 비 노이즈 데이터 1194개(실제 비노이즈 1186개 + 노이즈 8개)  \n",
    "\n",
    "### morph_condition V1 (2개)\n",
    "```\n",
    "ynat-v1_train_00858,5,한국MS 윈도10 IoT 에디션 출시…B2B 공략\n",
    "ynat-v1_train_01717,6,中 베이징서 H7N9 조류인플루엔자 환자 또 발생\n",
    "```\n",
    "\n",
    "### morph_condition V2 (5개)\n",
    "```\n",
    "ynat-v1_train_00982,4,회전 카메라 탑재한 갤럭시A80 SKT 단독출시…59만9천500원\n",
    "ynat-v1_train_01178,1,통신3사 5G 가입자 경쟁 불붙었다…LG V50 지원금 최대 77만원\n",
    "ynat-v1_train_01225,5,세계 첫 5G폰 갤럭시S10 5G 출고가 139만7천원 확정종합\n",
    "ynat-v1_train_01776,5,LGU 5G SA 상용화 준비…SA 기술 NSA 코어 장비에 연동 검증\n",
    "ynat-v1_train_01873,3,LG전자 스마트폰사업 10분기 적자…3Q 영업손실 3천753억종합\n",
    "```\n",
    "\n",
    "### sandwich (0개)\n",
    "\n",
    "### 문자 비율 기반 정규표현식 (4개)\n",
    "```\n",
    "ynat-v1_train_00191,4,MLB.com 다저스 3년 연속 WS 유력하지만 우승은 글쎄…\n",
    "ynat-v1_train_00741,6,게시판 SKT AI 콘퍼런스 ai.x 2019 개최\n",
    "ynat-v1_train_02249,0,클린턴재단 후폭풍인가…힐러리 vs 트럼프 지지율 3%p로 좁혀져\n",
    "ynat-v1_train_02399,2,재단법인화 tbs 초대 대표에 이강택 현 교통방송 대표\n",
    "```\n",
    "\n",
    "### 연속 특수문자 기반 (0개)\n",
    "\n",
    "### 특수문자 및 소문자 비율 기반 (0개)\n",
    "\n",
    "### 모델 기반 탐지 (0개)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 하나의 데이터로 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df1 = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(\"rule_based_noise.csv\"),\n",
    "        pd.read_csv(\"df_model_based_noise.csv\"),\n",
    "    ]\n",
    ")\n",
    "df2 = pd.read_csv(\"df_model_based_not_noise.csv\")\n",
    "\n",
    "# 각각의 데이터프레임에 is_noise 컬럼 추가\n",
    "df1[\"is_noise\"] = 1\n",
    "df2[\"is_noise\"] = 0\n",
    "\n",
    "# 두 데이터프레임 합치기\n",
    "df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "df_combined = df_combined[[\"ID\",\"text\",\"target\",\"is_noise\"]]\n",
    "df_combined = df_combined.sort_values(by=\"ID\", ascending=True)\n",
    "\n",
    "# 결과 저장\n",
    "df_combined.to_csv(\"noise_detected_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2800"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
