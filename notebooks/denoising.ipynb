{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2800"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/sujin-5_base_noise_detected.csv\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 의미 없는 문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>is_noise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ynat-v1_train_00000</td>\n",
       "      <td>정i :파1 미사z KT( 이용기간 2e 단] Q분종U</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ynat-v1_train_00001</td>\n",
       "      <td>K찰.국DLwo 로L3한N% 회장 2 T0&amp;}송=</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ynat-v1_train_00002</td>\n",
       "      <td>m 김정) 자주통일 새,?r열1나가야</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ynat-v1_train_00003</td>\n",
       "      <td>갤노트8 주말 27만대 개통…시장은 불법 보조금 얼룩</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ynat-v1_train_00004</td>\n",
       "      <td>pI美대선I앞두고 R2fr단 발] $비해 감시 강화</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID                            text  target  is_noise\n",
       "0  ynat-v1_train_00000  정i :파1 미사z KT( 이용기간 2e 단] Q분종U       4         1\n",
       "1  ynat-v1_train_00001     K찰.국DLwo 로L3한N% 회장 2 T0&}송=       3         1\n",
       "2  ynat-v1_train_00002            m 김정) 자주통일 새,?r열1나가야       2         1\n",
       "3  ynat-v1_train_00003   갤노트8 주말 27만대 개통…시장은 불법 보조금 얼룩       5         0\n",
       "4  ynat-v1_train_00004    pI美대선I앞두고 R2fr단 발] $비해 감시 강화       6         1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text 앞뒤에 그래픽/게시판/종합 등의 의미 없는 단어가 나오는 경우 삭제\n",
    "df[\"text\"] = df[\"text\"].replace(r\"^(그래픽|게시판)\", \"\", regex=True)\n",
    "df[\"text\"] = df[\"text\"].replace(r\"^(종합|1보|2보|3보)\", \"\", regex=True)\n",
    "df[\"text\"] = df[\"text\"].replace(r\"(종합|종합1보|종합2보|1보|2보|3보)$\", \"\", regex=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>is_noise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ynat-v1_train_00000</td>\n",
       "      <td>정i :파1 미사z KT( 이용기간 2e 단] Q분종U</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ynat-v1_train_00001</td>\n",
       "      <td>K찰.국DLwo 로L3한N% 회장 2 T0&amp;}송=</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ynat-v1_train_00002</td>\n",
       "      <td>m 김정) 자주통일 새,?r열1나가야</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ynat-v1_train_00003</td>\n",
       "      <td>갤노트8 주말 27만대 개통 시장은 불법 보조금 얼룩</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ynat-v1_train_00004</td>\n",
       "      <td>pI美대선I앞두고 R2fr단 발] $비해 감시 강화</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID                            text  target  is_noise\n",
       "0  ynat-v1_train_00000  정i :파1 미사z KT( 이용기간 2e 단] Q분종U       4         1\n",
       "1  ynat-v1_train_00001     K찰.국DLwo 로L3한N% 회장 2 T0&}송=       3         1\n",
       "2  ynat-v1_train_00002            m 김정) 자주통일 새,?r열1나가야       2         1\n",
       "3  ynat-v1_train_00003   갤노트8 주말 27만대 개통 시장은 불법 보조금 얼룩       5         0\n",
       "4  ynat-v1_train_00004    pI美대선I앞두고 R2fr단 발] $비해 감시 강화       6         1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 말줄임표: 띄어쓰기 처리 혹은 제거\n",
    "df[\"text\"] = df[\"text\"].str.replace(\"…\", \" \")\n",
    "df[\"text\"] = df[\"text\"].str.replace(\"...\", \"\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 노이즈 문장에서 특수문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# 영어, 숫자, 한글, 한자 제외 제거\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r\"[^a-zA-Z0-9\\sㄱ-ㅎㅏ-ㅣ가-힣\\u4e00-\\u9fff]\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>is_noise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ynat-v1_train_00000</td>\n",
       "      <td>정i 파1 미사z KT 이용기간 2e 단 Q분종U</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ynat-v1_train_00001</td>\n",
       "      <td>K찰국DLwo 로L3한N 회장 2 T0송</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ynat-v1_train_00002</td>\n",
       "      <td>m 김정 자주통일 새r열1나가야</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ynat-v1_train_00003</td>\n",
       "      <td>갤노트8 주말 27만대 개통 시장은 불법 보조금 얼룩</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ynat-v1_train_00004</td>\n",
       "      <td>pI美대선I앞두고 R2fr단 발 비해 감시 강화</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID                           text  target  is_noise\n",
       "0  ynat-v1_train_00000    정i 파1 미사z KT 이용기간 2e 단 Q분종U       4         1\n",
       "1  ynat-v1_train_00001         K찰국DLwo 로L3한N 회장 2 T0송       3         1\n",
       "2  ynat-v1_train_00002              m 김정 자주통일 새r열1나가야       2         1\n",
       "3  ynat-v1_train_00003  갤노트8 주말 27만대 개통 시장은 불법 보조금 얼룩       5         0\n",
       "4  ynat-v1_train_00004     pI美대선I앞두고 R2fr단 발 비해 감시 강화       6         1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 노이즈 데이터에만 함수 적용\n",
    "df.loc[df[\"is_noise\"] == 1, \"text\"] = df.loc[df[\"is_noise\"] == 1, \"text\"].apply(\n",
    "    remove_special_characters\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 한자는 한글 뜻을 함께 기입하여 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 ['美', '北', '中', '朴', '靑', '日', '與', '文', '英', '野', '佛', '伊', '獨', '反', '前', '軍', '硏', '對', '外', '社', '黃', '亞', '韓', '株', '車', '崔', '院', '金', '丁', '小', '和', '企', '安', '展', '檢', '親', '銀', '證', '先', '父', '南', '詩', '家', '大', '印', '阿', '故', '州', '重']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "# 한자 추출 함수 정의\n",
    "def extract_hanza(text):\n",
    "    # CJK 통합 한자 범위(U+4E00 ~ U+9FFF)에 해당하는 문자 추출\n",
    "    return \"\".join(re.findall(\"[\\u4e00-\\u9fff]\", text))\n",
    "\n",
    "\n",
    "# 문자 빈도 축정\n",
    "def get_char_frequency(text_list):\n",
    "    # 모든 문자를 하나의 리스트로 합치기\n",
    "    chars = [char for text in text_list for char in text]\n",
    "    # Counter를 사용하여 빈도 계산\n",
    "    char_counts = Counter(chars)\n",
    "    # 빈도순으로 정렬\n",
    "    return pd.Series(char_counts).sort_values(ascending=False)\n",
    "\n",
    "# 한자 빈도수 측정\n",
    "hanza = df[\"text\"].apply(extract_hanza)\n",
    "\n",
    "char_frequency = get_char_frequency(hanza)\n",
    "hanza = list(char_frequency.keys())\n",
    "print(len(hanza), hanza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hanza_hangul_meaning = [\n",
    "    ('美', '미국', '나라'), ('北', '북한', '나라'), ('中', '중국', '나라'),\n",
    "    ('朴', '박', '성씨'), ('靑', '청와대', '정치'), ('日', '일본', '나라'),\n",
    "    ('與', '여당', '정치'), ('文', '문','성씨'), ('英', '영국', '나라'),\n",
    "    ('野', '야당', '정치'), ('佛', '프랑스', '나라'), ('伊', '이탈리아', '나라'),\n",
    "    ('獨', '독일', '나라'), ('反', '반', '일반'), ('前', '전', '일반'),\n",
    "    ('軍', '군대', '일반'), ('硏', '연구원', '일반'), ('對', '대', '일반'),\n",
    "    ('外', '외', '일반'), ('社', '회사', '경제'), ('黃', '황','성씨'),\n",
    "    ('亞', '아시아', '나라'), ('韓', '한국', '나라'), ('株', '주식', '경제'),\n",
    "    ('車', '차', '일반'), ('崔', '최','성씨'), ('院', '위원', '일반'),\n",
    "    ('金', '김','성씨'), ('丁', '정','성씨'), ('小', '소', '일반'),\n",
    "    ('和', '네덜란드', '나라'), ('企', '기업', '경제'), ('安', '안','성씨'),\n",
    "    ('展', '전시', '일반'), ('檢', '검찰', '정치'), ('親', '친', '일반'),\n",
    "    ('銀', '은행', '경제'), ('證', '증권', '경제'), ('先', '선', '일반'),\n",
    "    ('父', '아버지', '일반'), ('南', '남한', '나라'), ('詩', '시', '일반'),\n",
    "    ('家', '집안', '일반'), ('大', '대', '일반'), ('印', '인도', '나라'),\n",
    "    ('阿', '아프리카', '나라'), ('故', '고', '일반'), ('州', '주', '나라'),\n",
    "    ('重', '중공업', '경제')\n",
    "]\n",
    "\n",
    "def annotate_hanzi_with_meaning(text):\n",
    "    for hanzi, hangul, _ in hanza_hangul_meaning:\n",
    "        text = text.replace(hanzi, f\"{hanzi}({hangul})\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>is_noise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ynat-v1_train_00000</td>\n",
       "      <td>정i 파1 미사z KT 이용기간 2e 단 Q분종U</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ynat-v1_train_00001</td>\n",
       "      <td>K찰국DLwo 로L3한N 회장 2 T0송</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ynat-v1_train_00002</td>\n",
       "      <td>m 김정 자주통일 새r열1나가야</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ynat-v1_train_00003</td>\n",
       "      <td>갤노트8 주말 27만대 개통 시장은 불법 보조금 얼룩</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ynat-v1_train_00004</td>\n",
       "      <td>pI美(미국)대선I앞두고 R2fr단 발 비해 감시 강화</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ynat-v1_train_00005</td>\n",
       "      <td>美(미국)성인 6명 중 1명꼴 배우자·연인 빚 떠안은 적 있다</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ynat-v1_train_00006</td>\n",
       "      <td>프로야구롯TKIAs광주 경기 y천취소</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ynat-v1_train_00007</td>\n",
       "      <td>아가메즈 33득점 우리카드 KB손해보험 완파 3위 굳</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ynat-v1_train_00008</td>\n",
       "      <td>朴(박)대통령 얼마나 많이 놀라셨어요 경주 지진현장 방문</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ynat-v1_train_00009</td>\n",
       "      <td>듀얼심 아이폰 하반기 출시설 솔솔 알뜰폰 기대감</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID                                text  target  is_noise\n",
       "0  ynat-v1_train_00000         정i 파1 미사z KT 이용기간 2e 단 Q분종U       4         1\n",
       "1  ynat-v1_train_00001              K찰국DLwo 로L3한N 회장 2 T0송       3         1\n",
       "2  ynat-v1_train_00002                   m 김정 자주통일 새r열1나가야       2         1\n",
       "3  ynat-v1_train_00003       갤노트8 주말 27만대 개통 시장은 불법 보조금 얼룩       5         0\n",
       "4  ynat-v1_train_00004      pI美(미국)대선I앞두고 R2fr단 발 비해 감시 강화       6         1\n",
       "5  ynat-v1_train_00005  美(미국)성인 6명 중 1명꼴 배우자·연인 빚 떠안은 적 있다       0         0\n",
       "6  ynat-v1_train_00006                프로야구롯TKIAs광주 경기 y천취소       1         1\n",
       "7  ynat-v1_train_00007       아가메즈 33득점 우리카드 KB손해보험 완파 3위 굳       4         0\n",
       "8  ynat-v1_train_00008     朴(박)대통령 얼마나 많이 놀라셨어요 경주 지진현장 방문       6         0\n",
       "9  ynat-v1_train_00009          듀얼심 아이폰 하반기 출시설 솔솔 알뜰폰 기대감       4         0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, \"text\"] = df[\"text\"].apply(annotate_hanzi_with_meaning)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"3_d_2800_hanzi_dictionary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 형태소 분석기로 노이즈 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/3_d_2800_hanzi_dictionary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1602\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>is_noise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ynat-v1_train_00000</td>\n",
       "      <td>정i 파1 미사z KT 이용기간 2e 단 Q분종U</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ynat-v1_train_00001</td>\n",
       "      <td>K찰국DLwo 로L3한N 회장 2 T0송</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ynat-v1_train_00002</td>\n",
       "      <td>m 김정 자주통일 새r열1나가야</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ynat-v1_train_00004</td>\n",
       "      <td>pI美(미국)대선I앞두고 R2fr단 발 비해 감시 강화</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ynat-v1_train_00006</td>\n",
       "      <td>프로야구롯TKIAs광주 경기 y천취소</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID                            text  target  is_noise\n",
       "0  ynat-v1_train_00000     정i 파1 미사z KT 이용기간 2e 단 Q분종U       4         1\n",
       "1  ynat-v1_train_00001          K찰국DLwo 로L3한N 회장 2 T0송       3         1\n",
       "2  ynat-v1_train_00002               m 김정 자주통일 새r열1나가야       2         1\n",
       "4  ynat-v1_train_00004  pI美(미국)대선I앞두고 R2fr단 발 비해 감시 강화       6         1\n",
       "6  ynat-v1_train_00006            프로야구롯TKIAs광주 경기 y천취소       1         1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[\"is_noise\"]==1]\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기 임포트\n",
    "from konlpy.tag import *\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pos_tagging\"] = df[\"text\"].copy()\n",
    "# 단어별 구분 없이 한 문장 통으로 하려면\n",
    "# df[\"pos_tagging\"] = df[\"pos_tagging\"].apply(lambda text: okt.pos(text))\n",
    "# 단어별로 형태소 분석 적용\n",
    "df[\"pos_tagging\"] = df[\"pos_tagging\"].apply(str.split)\n",
    "df[\"pos_tagging\"] = df[\"pos_tagging\"].apply(lambda li: [okt.pos(e) for e in li])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>is_noise</th>\n",
       "      <th>pos_tagging</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ynat-v1_train_00000</td>\n",
       "      <td>정i 파1 미사z KT 이용기간 2e 단 Q분종U</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[[(정, Noun), (i, Alpha)], [(파, Noun), (1, Numb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ynat-v1_train_00001</td>\n",
       "      <td>K찰국DLwo 로L3한N 회장 2 T0송</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[[(K, Alpha), (찰국, Noun), (DLwo, Alpha)], [(로,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ynat-v1_train_00002</td>\n",
       "      <td>m 김정 자주통일 새r열1나가야</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[[(m, Alpha)], [(김정, Noun)], [(자주, Noun), (통일,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ynat-v1_train_00004</td>\n",
       "      <td>pI美(미국)대선I앞두고 R2fr단 발 비해 감시 강화</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>[[(pI, Alpha), (美, Foreign), ((, Punctuation),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ynat-v1_train_00006</td>\n",
       "      <td>프로야구롯TKIAs광주 경기 y천취소</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[[(프로야구, Noun), (롯, Noun), (TKIAs, Alpha), (광주...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID                            text  target  is_noise  \\\n",
       "0  ynat-v1_train_00000     정i 파1 미사z KT 이용기간 2e 단 Q분종U       4         1   \n",
       "1  ynat-v1_train_00001          K찰국DLwo 로L3한N 회장 2 T0송       3         1   \n",
       "2  ynat-v1_train_00002               m 김정 자주통일 새r열1나가야       2         1   \n",
       "4  ynat-v1_train_00004  pI美(미국)대선I앞두고 R2fr단 발 비해 감시 강화       6         1   \n",
       "6  ynat-v1_train_00006            프로야구롯TKIAs광주 경기 y천취소       1         1   \n",
       "\n",
       "                                         pos_tagging  \n",
       "0  [[(정, Noun), (i, Alpha)], [(파, Noun), (1, Numb...  \n",
       "1  [[(K, Alpha), (찰국, Noun), (DLwo, Alpha)], [(로,...  \n",
       "2  [[(m, Alpha)], [(김정, Noun)], [(자주, Noun), (통일,...  \n",
       "4  [[(pI, Alpha), (美, Foreign), ((, Punctuation),...  \n",
       "6  [[(프로야구, Noun), (롯, Noun), (TKIAs, Alpha), (광주...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>is_noise</th>\n",
       "      <th>pos_tagging</th>\n",
       "      <th>pos_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ynat-v1_train_00000</td>\n",
       "      <td>정i 파1 미사z KT 이용기간 2e 단 Q분종U</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[[(정, Noun), (i, Alpha)], [(파, Noun), (1, Numb...</td>\n",
       "      <td>정 파 미사  이용기간  단 분종</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ynat-v1_train_00001</td>\n",
       "      <td>K찰국DLwo 로L3한N 회장 2 T0송</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[[(K, Alpha), (찰국, Noun), (DLwo, Alpha)], [(로,...</td>\n",
       "      <td>찰국 로한 회장  송</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ynat-v1_train_00002</td>\n",
       "      <td>m 김정 자주통일 새r열1나가야</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[[(m, Alpha)], [(김정, Noun)], [(자주, Noun), (통일,...</td>\n",
       "      <td>김정 자주통일 새열나가야</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ynat-v1_train_00004</td>\n",
       "      <td>pI美(미국)대선I앞두고 R2fr단 발 비해 감시 강화</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>[[(pI, Alpha), (美, Foreign), ((, Punctuation),...</td>\n",
       "      <td>美(미국)대선앞두고 단 발 비해 감시 강화</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ynat-v1_train_00006</td>\n",
       "      <td>프로야구롯TKIAs광주 경기 y천취소</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[[(프로야구, Noun), (롯, Noun), (TKIAs, Alpha), (광주...</td>\n",
       "      <td>프로야구롯광주 경기 천취소</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID                            text  target  is_noise  \\\n",
       "0  ynat-v1_train_00000     정i 파1 미사z KT 이용기간 2e 단 Q분종U       4         1   \n",
       "1  ynat-v1_train_00001          K찰국DLwo 로L3한N 회장 2 T0송       3         1   \n",
       "2  ynat-v1_train_00002               m 김정 자주통일 새r열1나가야       2         1   \n",
       "4  ynat-v1_train_00004  pI美(미국)대선I앞두고 R2fr단 발 비해 감시 강화       6         1   \n",
       "6  ynat-v1_train_00006            프로야구롯TKIAs광주 경기 y천취소       1         1   \n",
       "\n",
       "                                         pos_tagging            pos_processed  \n",
       "0  [[(정, Noun), (i, Alpha)], [(파, Noun), (1, Numb...       정 파 미사  이용기간  단 분종  \n",
       "1  [[(K, Alpha), (찰국, Noun), (DLwo, Alpha)], [(로,...              찰국 로한 회장  송  \n",
       "2  [[(m, Alpha)], [(김정, Noun)], [(자주, Noun), (통일,...            김정 자주통일 새열나가야  \n",
       "4  [[(pI, Alpha), (美, Foreign), ((, Punctuation),...  美(미국)대선앞두고 단 발 비해 감시 강화  \n",
       "6  [[(프로야구, Noun), (롯, Noun), (TKIAs, Alpha), (광주...           프로야구롯광주 경기 천취소  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def join_words(morphs_lists):\n",
    "    def filter_and_join_morphemes(morphological_results):\n",
    "        # Alpha와 Punctuation을 제외한 문자만 필터링\n",
    "        filtered_characters = [\n",
    "            word for word, pos in morphological_results if pos not in [\"Alpha\", \"Number\"]\n",
    "        ]\n",
    "        # 필터링된 문자들을 공백으로 구분하여 문자열로 결합\n",
    "        return \"\".join(filtered_characters)\n",
    "    return \" \".join(\n",
    "        [filter_and_join_morphemes(morphs_list) for morphs_list in morphs_lists]\n",
    "    )\n",
    "\n",
    "\n",
    "df[\"pos_processed\"] = df[\"pos_tagging\"].apply(join_words)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pos_processed\"] = df[\"pos_processed\"].str.strip()\n",
    "df[\"pos_processed\"] = df[\"pos_processed\"].apply(lambda x: re.sub(r\"\\s+\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"ID\", \"pos_processed\", \"target\",\"is_noise\"]].rename(columns={\"pos_processed\": \"text\"})\n",
    "df.to_csv(\"4_d_1602_only_noise.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 비 노이즈 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/3_d_2800_hanzi_dictionary.csv\")\n",
    "df = df[df[\"is_noise\"]==0]\n",
    "len(df)\n",
    "df.to_csv(\"5_d_1198_only_not_noise.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 비 노이즈 데이터 기반으로 BART 노이즈 복구 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv(\"../data/5_d_1198_only_not_noise.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "# 노이즈 문장에서 추출한 특수문자 목록\n",
    "special_characters = ['…', '.', '·', '%', '\"', '-', '(', '|', '?', ',', '}', ':', '&', '_', '{', '~', '#', '\\\\', '*', ')', '$', '=', '+', '`', ';', \"'\", '!', '@', '<', '/', '>', '[', ']', '^', '↑', '∼', '↓', '→', '㎜', '＋', 'ㆍ', '㎞', '㎡', 'ｍ', '②', '％', '㎝', '↔', '⅔', '㎏', '④', '③']\n",
    "\n",
    "def introduce_noise(text: str) -> str:\n",
    "    # 한글 문자 찾기\n",
    "    hangul_chars = re.findall(r'[가-힣]', text)\n",
    "    # 20%에 해당하는 개수 계산 (최소 1개)\n",
    "    num_noise = max(1, int(len(hangul_chars) * 0.2))\n",
    "    \n",
    "    # 변경할 인덱스 랜덤 선택\n",
    "    indexes_to_replace = random.sample(range(len(hangul_chars)), num_noise)\n",
    "    \n",
    "    # 텍스트 수정을 위한 리스트 변환\n",
    "    noisy_text = list(text)\n",
    "    \n",
    "    for index in indexes_to_replace:\n",
    "        # 랜덤 문자 생성 (숫자, 영어 대소문자, 정의된 특수문자)\n",
    "        random_char = random.choice(\n",
    "            [chr(x) for x in (\n",
    "                list(range(48, 58)) +     # 숫자 (0-9)\n",
    "                list(range(65, 91)) +     # 대문자 (A-Z)\n",
    "                list(range(97, 123))      # 소문자 (a-z)\n",
    "            )] + special_characters            # 정의된 특수문자\n",
    "        )\n",
    "        \n",
    "        # 한글 문자를 랜덤 문자로 교체\n",
    "        char_index = text.index(hangul_chars[index])\n",
    "        # noisy_text[char_index] = random_char\n",
    "        noisy_text[char_index] = \"\"\n",
    "        # 사용된 한글 문자 제거 (같은 문자 중복 변경 방지)\n",
    "        text = text[:char_index] + text[char_index + 1:]\n",
    "    \n",
    "    return ''.join(noisy_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>noised_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ynat-v1_train_00003</td>\n",
       "      <td>갤노트8 주말 27만대 개통 시장은 불법 보조금 얼룩</td>\n",
       "      <td>갤노트8 말 27만대 개통 시장은 불법 보금 얼룩</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ynat-v1_train_00005</td>\n",
       "      <td>美(미국)성인 6명 중 1명꼴 배우자·연인 빚 떠안은 적 있다</td>\n",
       "      <td>美(미국)성 6 중 1명 배우자·연인  떠안은 적 있다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ynat-v1_train_00007</td>\n",
       "      <td>아가메즈 33득점 우리카드 KB손해보험 완파 3위 굳</td>\n",
       "      <td>가메즈 33득점 우리드 KB손해보험완파 3위 굳</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ynat-v1_train_00008</td>\n",
       "      <td>朴(박)대통령 얼마나 많이 놀라셨어요 경주 지진현장 방문</td>\n",
       "      <td>朴(대통령 얼마나 많이 놀라셨어요 경주 지진현 문</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ynat-v1_train_00009</td>\n",
       "      <td>듀얼심 아이폰 하반기 출시설 솔솔 알뜰폰 기대감</td>\n",
       "      <td>얼심 아폰 하반기 출설솔솔 알뜰폰 기대감</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID                                text  \\\n",
       "0  ynat-v1_train_00003       갤노트8 주말 27만대 개통 시장은 불법 보조금 얼룩   \n",
       "1  ynat-v1_train_00005  美(미국)성인 6명 중 1명꼴 배우자·연인 빚 떠안은 적 있다   \n",
       "2  ynat-v1_train_00007       아가메즈 33득점 우리카드 KB손해보험 완파 3위 굳   \n",
       "3  ynat-v1_train_00008     朴(박)대통령 얼마나 많이 놀라셨어요 경주 지진현장 방문   \n",
       "4  ynat-v1_train_00009          듀얼심 아이폰 하반기 출시설 솔솔 알뜰폰 기대감   \n",
       "\n",
       "                      noised_text  \n",
       "0     갤노트8 말 27만대 개통 시장은 불법 보금 얼룩  \n",
       "1  美(미국)성 6 중 1명 배우자·연인  떠안은 적 있다  \n",
       "2      가메즈 33득점 우리드 KB손해보험완파 3위 굳  \n",
       "3     朴(대통령 얼마나 많이 놀라셨어요 경주 지진현 문  \n",
       "4          얼심 아폰 하반기 출설솔솔 알뜰폰 기대감  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"noised_text\"] = df_train[\"text\"].copy()\n",
    "df_train.loc[:,\"noised_text\"] = df_train[\"noised_text\"].apply(introduce_noise)\n",
    "df_train = df_train[[\"ID\",\"text\",\"noised_text\"]]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv(\"../data/4_d_1602_only_noise.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1602"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = list(df_test[\"text\"])\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class TextReconstructor:\n",
    "    def __init__(self, model_name=\"gogamza/kobart-base-v2\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def reconstruct(self, corrupted_text, max_length=32):\n",
    "        # token_type_ids를 False로 설정\n",
    "        inputs = self.tokenizer(corrupted_text, \n",
    "                              return_tensors=\"pt\", \n",
    "                              truncation=True, \n",
    "                              max_length=max_length,\n",
    "                              return_token_type_ids=False)  # 이 부분 추가\n",
    "        inputs = inputs.to(self.device)\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=10,\n",
    "            no_repeat_ngram_size=2,\n",
    "            top_k=10,\n",
    "            top_p=0.95,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        reconstructed = self.tokenizer.decode(outputs[0], \n",
    "                                            skip_special_tokens=True)\n",
    "        return reconstructed\n",
    "\n",
    "\n",
    "\n",
    "class TextReconstructionDataset(Dataset):\n",
    "    def __init__(self, corrupted_texts, original_texts, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        print(\"토크나이징 데이터...\")\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for corrupt_text, orig_text in tqdm(zip(corrupted_texts, original_texts), \n",
    "                                          total=len(corrupted_texts),\n",
    "                                          desc=\"데이터셋 준비중\"):\n",
    "            # return_token_type_ids=False 추가\n",
    "            self.inputs.append(\n",
    "                self.tokenizer(\n",
    "                    corrupt_text,\n",
    "                    truncation=True,\n",
    "                    max_length=max_length,\n",
    "                    padding='max_length',\n",
    "                    return_tensors='pt',\n",
    "                    return_token_type_ids=False\n",
    "                )\n",
    "            )\n",
    "            self.targets.append(\n",
    "                self.tokenizer(\n",
    "                    orig_text,\n",
    "                    truncation=True,\n",
    "                    max_length=max_length,\n",
    "                    padding='max_length',\n",
    "                    return_tensors='pt',\n",
    "                    return_token_type_ids=False\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.inputs[idx][\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": self.inputs[idx][\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": self.targets[idx][\"input_ids\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "\n",
    "def train_model(model, train_dataset, valid_dataset):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"../outputs\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        save_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"none\",\n",
    "        # 진행 상황 표시 활성화\n",
    "        disable_tqdm=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset\n",
    "    )\n",
    "    \n",
    "    print(\"모델 학습 시작...\")\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "def prepare_and_train(df):\n",
    "    print(\"데이터 분할 중...\")\n",
    "    train_df, valid_df = train_test_split(\n",
    "        df, \n",
    "        test_size=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"모델 초기화 중...\")\n",
    "    model_name = \"gogamza/kobart-base-v2\"\n",
    "    reconstructor = TextReconstructor(model_name)\n",
    "    \n",
    "    print(\"학습 데이터셋 생성 중...\")\n",
    "    train_dataset = TextReconstructionDataset(\n",
    "        corrupted_texts=train_df['noised_text'].tolist(),\n",
    "        original_texts=train_df['text'].tolist(),\n",
    "        tokenizer=reconstructor.tokenizer\n",
    "    )\n",
    "    \n",
    "    print(\"검증 데이터셋 생성 중...\")\n",
    "    valid_dataset = TextReconstructionDataset(\n",
    "        corrupted_texts=valid_df['noised_text'].tolist(),\n",
    "        original_texts=valid_df['text'].tolist(),\n",
    "        tokenizer=reconstructor.tokenizer\n",
    "    )\n",
    "    \n",
    "    train_model(reconstructor.model, train_dataset, valid_dataset)\n",
    "    \n",
    "    return reconstructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 프로세스 시작...\n",
      "데이터 분할 중...\n",
      "모델 초기화 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터셋 생성 중...\n",
      "토크나이징 데이터...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "데이터셋 준비중: 100%|██████████| 1078/1078 [00:00<00:00, 3644.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터셋 생성 중...\n",
      "토크나이징 데이터...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "데이터셋 준비중: 100%|██████████| 120/120 [00:00<00:00, 3650.55it/s]\n",
      "/opt/conda/envs/main/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 학습 시작...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='405' max='405' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [405/405 01:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.214600</td>\n",
       "      <td>1.705800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.572300</td>\n",
       "      <td>0.211874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.154700</td>\n",
       "      <td>0.191062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/main/lib/python3.10/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'forced_eos_token_id': 1}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 중...\n",
      "\n",
      "=== 테스트 결과 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "print(\"학습 프로세스 시작...\")\n",
    "reconstructor = prepare_and_train(df)\n",
    "\n",
    "print(\"모델 저장 중...\")\n",
    "reconstructor.model.save_pretrained(\"./reconstructor/\")\n",
    "reconstructor.tokenizer.save_pretrained(\"./reconstructor\")\n",
    "\n",
    "# 테스트\n",
    "print(\"\\n=== 테스트 결과 ===\")\n",
    "result_data = []\n",
    "for text in test_data:\n",
    "    reconstructed = reconstructor.reconstruct(text)\n",
    "    result_data.append(reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>is_noise</th>\n",
       "      <th>denoised_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ynat-v1_train_00000</td>\n",
       "      <td>정 파 미사 이용기간 단 분종</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>정정파 장거리 미사 이용기간 단기간 내 분종종기간단 추진기간도 단시종점 내달 중순 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ynat-v1_train_00001</td>\n",
       "      <td>찰국 로한 회장 송</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>찰스 로한 회장 송찰찰 찰 찰즈라엘 총리 송 찰찰라엘 조지아로한 회장을 송송 찰옥국...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ynat-v1_train_00002</td>\n",
       "      <td>김정 자주통일 새열나가야</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>김정은 자주통일 새 시대 열어가야 한다고 선언했다.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ynat-v1_train_00004</td>\n",
       "      <td>美(미국)대선앞두고 단 발 비해 감시 강화</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>美(미국)대선앞두고 단발성 비난에 비해 감시 강화키로</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ynat-v1_train_00006</td>\n",
       "      <td>프로야구롯광주 경기 천취소</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>프로야구 롯데롯데롯광주 경기 천취소 승리소식삼진취소로</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID                     text  target  is_noise  \\\n",
       "0  ynat-v1_train_00000         정 파 미사 이용기간 단 분종       4         1   \n",
       "1  ynat-v1_train_00001               찰국 로한 회장 송       3         1   \n",
       "2  ynat-v1_train_00002            김정 자주통일 새열나가야       2         1   \n",
       "3  ynat-v1_train_00004  美(미국)대선앞두고 단 발 비해 감시 강화       6         1   \n",
       "4  ynat-v1_train_00006           프로야구롯광주 경기 천취소       1         1   \n",
       "\n",
       "                                       denoised_text  \n",
       "0  정정파 장거리 미사 이용기간 단기간 내 분종종기간단 추진기간도 단시종점 내달 중순 ...  \n",
       "1  찰스 로한 회장 송찰찰 찰 찰즈라엘 총리 송 찰찰라엘 조지아로한 회장을 송송 찰옥국...  \n",
       "2                     김정은 자주통일 새 시대 열어가야 한다고 선언했다.\\n  \n",
       "3                      美(미국)대선앞두고 단발성 비난에 비해 감시 강화키로  \n",
       "4                      프로야구 롯데롯데롯광주 경기 천취소 승리소식삼진취소로  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"denoised_text\"] = result_data\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[[\"ID\", \"denoised_text\", \"target\",\"is_noise\"]].rename(columns={\"denoised_text\": \"text\"})\n",
    "df_test[\"text\"] = df_test[\"text\"].str.replace(\"\\n\", \"\")\n",
    "df_test.to_csv(\"6_d_1602_kobart-denoise.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 데이터 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv(\"../data/5_d_1198_only_not_noise.csv\")\n",
    "df2 = pd.read_csv(\"../data/6_d_1602_kobart_denoise.csv\")\n",
    "\n",
    "df = pd.concat([df1,df2], axis=0)\n",
    "df[\"text\"] = df[\"text\"].str.strip()\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: re.sub(r\"\\s+\", \" \", x))\n",
    "\n",
    "print(len(df))\n",
    "df = df.sort_values(by=\"ID\", ascending=True)\n",
    "df = df[['ID', 'text', 'target']]\n",
    "df.to_csv(\"7_d_2800_kobart_denoise.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. cleanlab 라벨링 및 정상 라벨링 복구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정렬\n",
    "df = pd.read_csv(\"../data/7_dr_2800_kobart_cleanlab.csv\")\n",
    "df = df.sort_values(by=\"ID\", ascending=True)\n",
    "df.to_csv(\"../data/7_dr_2800_kobart_cleanlab.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"../data/7_dr_2800_kobart_cleanlab.csv\")\n",
    "df2 = pd.read_csv(\"../data/6_d_1602_kobart_denoise.csv\")\n",
    "\n",
    "# merge를 사용하여 ID 기준으로 데이터프레임 결합\n",
    "df1 = df1.merge(df2[['ID', 'target']], on='ID', how='left', suffixes=('', '_new'))\n",
    "df1['target'] = df1['target_new'].combine_first(df1['target'])\n",
    "df1 = df1.drop(columns=['target_new'])\n",
    "\n",
    "df1 = df1.sort_values(by=\"ID\", ascending=True)\n",
    "df1.to_csv(\"../data/7_dr_2800_kobart_cleanlab_update.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 바꾸기 전 라벨과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_before = pd.read_csv(\"../data/7_d_2800_kobart_denoise.csv\")\n",
    "df_after = pd.read_csv(\"../data/7_dr_2800_kobart_cleanlab_update.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target_before</th>\n",
       "      <th>target_after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ynat-v1_train_00003</td>\n",
       "      <td>갤노트8 주말 27만대 개통 시장은 불법 보조금 얼룩</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ynat-v1_train_00005</td>\n",
       "      <td>美(미국)성인 6명 중 1명꼴 배우자·연인 빚 떠안은 적 있다</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ynat-v1_train_00007</td>\n",
       "      <td>아가메즈 33득점 우리카드 KB손해보험 완파 3위 굳</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ynat-v1_train_00008</td>\n",
       "      <td>朴(박)대통령 얼마나 많이 놀라셨어요 경주 지진현장 방문</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ynat-v1_train_00011</td>\n",
       "      <td>NH투자 1월 옵션 만기일 매도 우세</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID                                text  target_before  \\\n",
       "3   ynat-v1_train_00003       갤노트8 주말 27만대 개통 시장은 불법 보조금 얼룩              5   \n",
       "5   ynat-v1_train_00005  美(미국)성인 6명 중 1명꼴 배우자·연인 빚 떠안은 적 있다              0   \n",
       "7   ynat-v1_train_00007       아가메즈 33득점 우리카드 KB손해보험 완파 3위 굳              4   \n",
       "8   ynat-v1_train_00008     朴(박)대통령 얼마나 많이 놀라셨어요 경주 지진현장 방문              6   \n",
       "11  ynat-v1_train_00011                NH투자 1월 옵션 만기일 매도 우세              1   \n",
       "\n",
       "    target_after  \n",
       "3            4.0  \n",
       "5            6.0  \n",
       "7            1.0  \n",
       "8            2.0  \n",
       "11           5.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 두 데이터프레임을 ID와 text 컬럼을 기준으로 병합\n",
    "df_merged = pd.merge(df_before, df_after, on=[\"ID\", \"text\"], suffixes=(\"_before\", \"_after\"))\n",
    "different_targets = df_merged[df_merged[\"target_before\"] != df_merged[\"target_after\"]]\n",
    "result_df = different_targets[['ID', 'text', 'target_before', 'target_after']]\n",
    "\n",
    "print(len(result_df))\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"label_diff.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO \n",
    "CleanLab 첫 시도에서 라벨 오류(= 비 노이즈) 데이터는 기존 틀린 라벨 그대로 예측할 확률에 패널티를 크게 주는 방법을 생각할 수 있다.    \n",
    "그럼 라벨이 틀리지 않은 200개는 어떻게 구별하는가? -> 기존 target과 새롭게 예측한 target이 같은 것 중에서 확률이 높은 순서대로 TOP 200을 뽑아서 추론할 수 있을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
